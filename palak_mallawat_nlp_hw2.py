# -*- coding: utf-8 -*-
"""Palak_Mallawat_NLP_HW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19IoOxXY44_rEnMSlAGEU5_H_rlPobwCC
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_table('/content/drive/MyDrive/data/train', header=None)
df['freq_count']=1
sdf= df['freq_count'].groupby(df[1])
df=pd.DataFrame(sdf.count())
sdf=df.sort_values(by=['freq_count'])
sdf = sdf.reset_index()
sdf.columns = ['token', 'freq_count']
sdf.loc[sdf['freq_count'] < 4, 'token'] = '< unk >'
aggregation_functions = {'freq_count': 'sum'}
df_new = sdf.groupby(sdf['token']).aggregate(aggregation_functions)
df_new.sort_values(by=['freq_count'], ascending=False,inplace=True)
df = df_new.drop(['< unk >'])
df = df.reset_index()
new_row=pd.DataFrame({'token':'< unk >','freq_count': 42044},index=[0])
df = pd.concat([new_row, df]).reset_index(drop = True)
df

df.to_csv("vocab.txt", sep="\t", header=None)
print(df.shape[0])

"""### Q)What is the selected threshold for unknown words replacement? 
A)The threshold is 3, i.e. the words equal to or less than 3 are converted to tag < unk >

### Q)What is the total size of your vocabulary 
13751

### Q) what is the total occurrences of the special token ‘< unk >’ after replacement?
A) 42044

# Task 2
"""

df_2 = pd.read_table('/content/drive/MyDrive/data/train', header=None, skip_blank_lines=False)

import numpy as np
df_2.rename(columns = {0:'index', 1:'token', 2:'pos'}, inplace = True)
df_2['freq_count']=df_2.groupby(['token'])['token'].transform('count')
df_2.loc[df_2['freq_count'] < 4, 'token'] = '< unk >'
df_2['freq_count']=df_2.groupby(['token'])['token'].transform('count')
df_2['pos'] = df_2['pos']. replace(np. nan, '<start>')
pos_trans = df_2['pos'].shift(-1)
df_2['pos_trans']=pd.Series(pos_trans)
df_2['final_trans']=df_2['pos']+ ', '+df_2['pos_trans']
trans_dict= df_2['final_trans'].value_counts().to_dict()
df_2['pos_emu']=df_2['pos']+ ', '+df_2['token']
emm_dict= df_2['pos_emu'].value_counts().to_dict()
pos_cnt = df_2['pos'].value_counts().to_dict()

pos_tags=df_2.pos.unique().tolist()
unique_words=df_2.token.unique().tolist()

transition={}
for i in pos_tags:
  for j in pos_tags:
    # print(i, j)
    if (i+", "+j) in trans_dict:
      # print(trans_dict[i,j])
      # print(pos_cnt[i])
      transition.update({i+', '+j: trans_dict[i+", "+j]/pos_cnt[i]})
    else:
      transition.update({i+', '+j : 0})

emmision={}
for i in pos_tags:
  for j in unique_words:
    if(j!='nan'):
      if (str(i)+', '+str(j)) in emm_dict:
        emmision.update({str(i)+', '+str(j): emm_dict[str(i)+', '+str(j)]/pos_cnt[i]})
      else:
        emmision.update({str(i)+', '+str(j) : 0})

print(len(transition))
print(len(emmision))

import json
with open("/content/drive/MyDrive/data/hmm.json", "w") as outfile:
    json.dump(transition, outfile)
    outfile.write('\n')
    json.dump(emmision, outfile)

transition_matrix = [[0]*len(pos_tags) for i in range(len(pos_tags))]
for i, j in transition.items():
  a,b = i.split(", ")
  transition_matrix[pos_tags.index(a)][pos_tags.index(b)]=j

emission_matrix = [[0]*len(unique_words) for i in range(len(pos_tags))]
for i,j in emmision.items():
  
  a,b=i.split(', ')
  # print(a, b)
  if(b!='nan'):
  
    emission_matrix[pos_tags.index(a)][unique_words.index(b)]=j

"""### Q)How many transition and emission parameters in your HMM?
A)Transition: 2116
Emission: 632592

# Task 3
"""

import numpy as np, numpy.random
df_dev=pd.read_table('/content/drive/MyDrive/data/dev', header=None)
df_dev.columns = ['index', 'word','pos']
ran = np.random.dirichlet(np.ones(46)*1000)
ran= ran.tolist()
devpos_tags = df_dev['pos'].unique().tolist()
dev_word_tag = df_dev['word'].unique().tolist()
start = dict(zip(pos_tags, ran))

sentences = []
sentence = []
i = 1
for row in df_dev.itertuples():
    if(row.index == 1 and i == 0):
        sentences.append(sentence)
        sentence = []
    i = 0
    sentence.append((row.word, row.pos))
sentences.append(sentence)

sequences = []
total = []
for sentence in sentences:
    prev = None
    curr_seq = []
    score = []
    for i in range(len(sentence)):
        best_score = -1
        word=sentence[i][0]
        
        for j in range(len(pos_tags)):
            curr_score = 1
            tag=pos_tags[j]
            if i == 0:
                curr_score = curr_score* start[tag]
                if str(tag + ", " + word) in emmision:
                  curr_score = curr_score* emmision[tag + ", " + word]
                else:
                  curr_score = curr_score* emmision[tag + ", " + "< unk >"]
            else:
                if str(prev  + ", " + tag) in transition:
                    curr_score = curr_score* transition[prev  + ", " + tag]
            
                if str(tag + ", " + word) in emmision:
                    curr_score = curr_score* emmision[tag + ", " + word]
                else:
                    curr_score = curr_score* emmision[tag + ", " + "< unk >"]
            
            if(curr_score > best_score):
                best_score = curr_score
                highest_prob_tag = tag
                
        prev= highest_prob_tag
        curr_seq.append(prev)
        score.append(best_score)
    sequences.append(curr_seq)
    total.append(score)

count = 0
corr_tag_count = 0
for i in range(len(sentences)):
    for j in range(len(sentences[i])):

        if(sequences[i][j] == sentences[i][j][1]):
            corr_tag_count += 1
        count +=1

acc = corr_tag_count / count
print(acc*100)

df_test=pd.read_table('/content/drive/MyDrive/data/test', header=None)
df_test.columns = ['index', 'word']

sentences = []
sentence = []
i = 1
for row in df_test.itertuples():
    if(row.index == 1 and i == 0):
        sentences.append(sentence)
        sentence = []
    i = 0
    sentence.append(row.word)
sentences.append(sentence)

sequences = []
total = []
for sentence in sentences:
    prev = None
    curr_seq = []
    score = []
    for i in range(len(sentence)):
        best_score = -1
        word=sentence[i][0]
        
        for j in range(len(pos_tags)):
            curr_score = 1
            tag=pos_tags[j]
            if i == 0:
                curr_score = curr_score* start[tag]
                if str(tag + ", " + word) in emmision:
                  curr_score = curr_score* emmision[tag + ", " + word]
                else:
                  curr_score = curr_score* emmision[tag + ", " + "< unk >"]
            else:
                if str(prev  + ", " + tag) in transition:
                    curr_score = curr_score* transition[prev  + ", " + tag]
            
                if str(tag + ", " + word) in emmision:
                    curr_score = curr_score* emmision[tag + ", " + word]
                else:
                    curr_score = curr_score* emmision[tag + ", " + "< unk >"]
            
            if(curr_score > best_score):
                highest_prob_tag = tag
                best_score = curr_score
                
        score.append(best_score)       
        prev= highest_prob_tag
        curr_seq.append(prev)
        
    sequences.append(curr_seq)
    total.append(score)

result=[]
for i in range(len(sentences)):
    for j in range(len(sentences[i])):
        result.append(sequences[i][j])

df_test['result']= result
df_test.to_csv("/content/drive/MyDrive/data/greedy.out", sep="\t", header=None)

"""### What is the accuracy on the dev data?
92.14604456317164%

# Task 4
"""

df_4=pd.read_table('/content/drive/MyDrive/data/dev', header=None)
df_4.columns = ['index', 'word','pos']

sentences = []
sentence = []
i= 1
for row in df_4.itertuples():
    if(row.index== 1 and i == 0):
        sentences.append(sentence)
        sentence = []
    i = 0
    sentence.append((row.word, row.pos))
sentences.append(sentence)

ran = np.random.dirichlet(np.ones(46)*1000)
ran= ran.tolist()
start = dict(zip(pos_tags, ran))

iteration=[]
prev=[]
for sen in sentences:
    prev_probs = []
    itr = {}
    for t in pos_tags:
      
      if str( t + ", " + sen[0][0]) in emmision:
        prev_probs.append(start[t] * emmision[t + ", " + sen[0][0]])

      else:
        prev_probs.append(start[t] * emmision[t + ", " + "< unk >"])

    for i, s in enumerate(sen):
      if i == 0: continue

      word = s[0]
      probs = [None] * len(pos_tags)

      for j,tag in enumerate(pos_tags):
        max = -1
        val = 1

        for k, p in enumerate(prev_probs):
          curr_tag=pos_tags[k]

          if str(curr_tag + ", " + tag) in transition and str(tag + ", " + word) not in emmision:
            val = p * transition[curr_tag + ", " + tag] * emmision[tag + ", " + "< unk >"]
            

          else:
            val = p * transition[curr_tag + ", " + tag] * emmision[tag + ", " + word]
            

          if(max < val):
            itr[str(i) + ", " + tag] = [curr_tag, val]
            max = val
            
        probs[j] = max
      for index, x in enumerate(probs):
          prev_probs[index]=x
    iteration.append(itr)
    prev.append(prev_probs)

#to backtrack for the best seq
def backtrack(pos_tags, itr, prev_probs):

    
    n = len(itr) // len(pos_tags)
    a= []
    b = []
    x = pos_tags[np.argmax(np.asarray(prev_probs))]
    a.append(x)

    for i in range(n, 0, -1):
        x = itr[str(i) + ', ' + x][0]
        val = itr[str(i) + ', ' + x][1]
        a = [x] + a
        b =  [val] + b
    
    return a,b

x = []
y= []
for itr, prev_probs in zip( iteration,prev):

    a, b = backtrack(pos_tags, itr, prev_probs)
    x.append(a)
    y.append(b)

#Accuracy
count = 0
corr_tag_count = 0
for i in range(len(sentences)):
    for j in range(len(sentences[i])):

        if(x[i][j] == sentences[i][j][1]):
            corr_tag_count += 1
        count +=1

acc = corr_tag_count / count
print(acc*100)



"""test data"""

sentences = []
sentence = []
i= 1
for row in df_test.itertuples():
    if(row.index== 1 and i == 0):
        sentences.append(sentence)
        sentence = []
    i= 0
    sentence.append(row.word)
sentences.append(sentence)

for sen in sentences:
    prev_probs = []
    for t in pos_tags:
      if str( t + ", " + sen[0][0]) in emmision:
        prev_probs.append(start[t] * emmision[t + ", " + sen[0][0]])
      else:
        prev_probs.append(start[t] * emmision[t + ", " + "< unk >"])

iteration=[]
prev=[]
for sen in sentences:
    prev_probs = []
    itr = {}
    for t in pos_tags:
      
      if str( t + ", " + sen[0][0]) in emmision:
        prev_probs.append(start[t] * emmision[t + ", " + sen[0][0]])

      else:
        prev_probs.append(start[t] * emmision[t + ", " + "< unk >"])

    for i, s in enumerate(sen):
      if i == 0: continue

      word = s[0]
      probs = [None] * len(pos_tags)

      for j,tag in enumerate(pos_tags):
        max = -1
        val = 1

        for k, p in enumerate(prev_probs):
          curr_tag=pos_tags[k]

          if str(curr_tag + ", " + tag) in transition and str(tag + ", " + word) not in emmision:
            val = p * transition[curr_tag + ", " + tag] * emmision[tag + ", " + "< unk >"]
            

          else:
            val = p * transition[curr_tag + ", " + tag] * emmision[tag + ", " + word]
            

          if(max < val):
            itr[str(i) + ", " + tag] = [curr_tag, val]
            max = val
            
        probs[j] = max
      for index, x in enumerate(probs):
          prev_probs[index]=x
    iteration.append(itr)
    prev.append(prev_probs)

x = []
y= []
for itr, prev_probs in zip(iteration,prev):

    a, b = backtrack(pos_tags, itr, prev_probs)
    x.append(a)
    y.append(b)

result=[]
for i in range(len(sentences)):
    for j in range(len(sentences[i])):
        result.append(x[i][j])

df_test['result']=result
df_test.to_csv("/content/drive/MyDrive/data/viterbi.out", sep="\t", header=None)

"""### Q)What is the accuracy on the dev data? 
A) 93.64792665897639%
"""